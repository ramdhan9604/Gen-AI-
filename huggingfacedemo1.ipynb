{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-27T13:14:07.245474Z","iopub.execute_input":"2024-10-27T13:14:07.245780Z","iopub.status.idle":"2024-10-27T13:14:20.691568Z","shell.execute_reply.started":"2024-10-27T13:14:07.245742Z","shell.execute_reply":"2024-10-27T13:14:20.690481Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"!nvidia-smi","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:14:46.957261Z","iopub.execute_input":"2024-10-27T13:14:46.957657Z","iopub.status.idle":"2024-10-27T13:14:48.092092Z","shell.execute_reply.started":"2024-10-27T13:14:46.957618Z","shell.execute_reply":"2024-10-27T13:14:48.091137Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Sun Oct 27 13:14:47 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n| N/A   52C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n| N/A   53C    P8             10W /   70W |       1MiB /  15360MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hugging Face Tasks","metadata":{}},{"cell_type":"markdown","source":"## NLP Tasks","metadata":{}},{"cell_type":"markdown","source":"### Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:17:19.130967Z","iopub.execute_input":"2024-10-27T13:17:19.131992Z","iopub.status.idle":"2024-10-27T13:17:38.001828Z","shell.execute_reply.started":"2024-10-27T13:17:19.131934Z","shell.execute_reply":"2024-10-27T13:17:38.000911Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"classifier = pipeline(\"text-classification\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:23:55.223489Z","iopub.execute_input":"2024-10-27T13:23:55.224241Z","iopub.status.idle":"2024-10-27T13:23:55.555439Z","shell.execute_reply.started":"2024-10-27T13:23:55.224200Z","shell.execute_reply":"2024-10-27T13:23:55.554622Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"result = classifier(\"I was so not happy with the last mission impossible movie\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:23:58.282160Z","iopub.execute_input":"2024-10-27T13:23:58.282541Z","iopub.status.idle":"2024-10-27T13:23:58.351104Z","shell.execute_reply.started":"2024-10-27T13:23:58.282505Z","shell.execute_reply":"2024-10-27T13:23:58.350233Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(result)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:23:59.123265Z","iopub.execute_input":"2024-10-27T13:23:59.124203Z","iopub.status.idle":"2024-10-27T13:23:59.129086Z","shell.execute_reply.started":"2024-10-27T13:23:59.124162Z","shell.execute_reply":"2024-10-27T13:23:59.128079Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"[{'label': 'NEGATIVE', 'score': 0.9997795224189758}]\n","output_type":"stream"}]},{"cell_type":"code","source":"pipeline(task=\"sentiment-analysis\")(\"I was confused with the Barbie Movie\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:27:23.210252Z","iopub.execute_input":"2024-10-27T13:27:23.210657Z","iopub.status.idle":"2024-10-27T13:27:23.578941Z","shell.execute_reply.started":"2024-10-27T13:27:23.210618Z","shell.execute_reply":"2024-10-27T13:27:23.577941Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[{'label': 'NEGATIVE', 'score': 0.9992005228996277}]"},"metadata":{}}]},{"cell_type":"code","source":"pipeline(task=\"sentiment-analysis\")\\\n                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n                                      Lots of them Looks very Promising. \\\n                                      I am not sure if we CAN actually Evaluate LLMs. \\\n                                      There is still lots to do.\\\n                                      Don't you think?\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:28:09.935948Z","iopub.execute_input":"2024-10-27T13:28:09.936819Z","iopub.status.idle":"2024-10-27T13:28:10.313107Z","shell.execute_reply.started":"2024-10-27T13:28:09.936781Z","shell.execute_reply":"2024-10-27T13:28:10.312040Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9964345693588257}]"},"metadata":{}}]},{"cell_type":"code","source":"pipeline(task=\"sentiment-analysis\",model=\"facebook/bart-large-mnli\")\\\n                                      (\"Everyday lots of LLMs papers are published about LLMs Evlauation. \\\n                                      Lots of them Looks very Promising. \\\n                                      I am not sure if we CAN actually Evaluate LLMs. \\\n                                      There is still lots to do.\\\n                                      Don't you think?\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:29:25.533114Z","iopub.execute_input":"2024-10-27T13:29:25.533599Z","iopub.status.idle":"2024-10-27T13:29:38.997502Z","shell.execute_reply.started":"2024-10-27T13:29:25.533558Z","shell.execute_reply":"2024-10-27T13:29:38.996282Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.15k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42186080885a404bb2ac96424a3d5cda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5759aa27aa440d88ba07509a8ef8f84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4675036c152544c7b530bf36c9b2ab4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94654c9ab704bf98340c23efc74658e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ebc183adb84b57867da59325109b7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47c934ead334445db96ef9a81a02ab96"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"[{'label': 'neutral', 'score': 0.7693328261375427}]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Batch Sentiment Analysis","metadata":{}},{"cell_type":"code","source":"classifier = pipeline(task=\"sentiment-analysis\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:31:23.898119Z","iopub.execute_input":"2024-10-27T13:31:23.898533Z","iopub.status.idle":"2024-10-27T13:31:24.205470Z","shell.execute_reply.started":"2024-10-27T13:31:23.898493Z","shell.execute_reply":"2024-10-27T13:31:24.204522Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"task_list = [\"I really like Autoencoders, best models for Anomaly Detection\", \\\n            \"I am not sure if we CAN actually Evaluate LLMs.\", \\\n            \"PassiveAgressive is the name of a Linear Regression Model that so many people do not know.\",\\\n            \"I hate long Meetings.\"]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:31:41.491284Z","iopub.execute_input":"2024-10-27T13:31:41.491763Z","iopub.status.idle":"2024-10-27T13:31:41.496722Z","shell.execute_reply.started":"2024-10-27T13:31:41.491724Z","shell.execute_reply":"2024-10-27T13:31:41.495626Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"classifier(task_list)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:32:05.793709Z","iopub.execute_input":"2024-10-27T13:32:05.794754Z","iopub.status.idle":"2024-10-27T13:32:05.947129Z","shell.execute_reply.started":"2024-10-27T13:32:05.794699Z","shell.execute_reply":"2024-10-27T13:32:05.946147Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"[{'label': 'POSITIVE', 'score': 0.9978686571121216},\n {'label': 'NEGATIVE', 'score': 0.9995476603507996},\n {'label': 'NEGATIVE', 'score': 0.9983084201812744},\n {'label': 'NEGATIVE', 'score': 0.9969881176948547}]"},"metadata":{}}]},{"cell_type":"code","source":"classifier = pipeline(task=\"sentiment-analysis\",model=\"facebook/bart-large-mnli\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:57:33.402756Z","iopub.execute_input":"2024-10-27T13:57:33.403651Z","iopub.status.idle":"2024-10-27T13:57:35.721921Z","shell.execute_reply.started":"2024-10-27T13:57:33.403608Z","shell.execute_reply":"2024-10-27T13:57:35.721110Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"classifier(task_list)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:57:35.723867Z","iopub.execute_input":"2024-10-27T13:57:35.724287Z","iopub.status.idle":"2024-10-27T13:57:43.633875Z","shell.execute_reply.started":"2024-10-27T13:57:35.724236Z","shell.execute_reply":"2024-10-27T13:57:43.632729Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"[{'label': 'neutral', 'score': 0.9760262370109558},\n {'label': 'neutral', 'score': 0.9137762784957886},\n {'label': 'neutral', 'score': 0.7985684871673584},\n {'label': 'neutral', 'score': 0.9845702052116394}]"},"metadata":{}}]},{"cell_type":"markdown","source":"### Text Generation","metadata":{}},{"cell_type":"code","source":"text_generator = pipeline(task=\"text-generation\",model=\"distilbert/distilgpt2\")","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:57:43.635866Z","iopub.execute_input":"2024-10-27T13:57:43.636227Z","iopub.status.idle":"2024-10-27T13:57:51.263929Z","shell.execute_reply.started":"2024-10-27T13:57:43.636192Z","shell.execute_reply":"2024-10-27T13:57:51.263016Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd1d5d1c52e24b44b4a9d59f7e94acb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a45d0c7421040ea86f32dce6f2d593d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d07fadab38a4580bd75d17561dcf6d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1daa0662c2244df5a07aff1ebdccea01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b4570f26baf486c996bb1821fa6e4ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07d4daa9c0304a4da82dee82941b386f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed2782d6264846f0b1d7bdf590c476f6"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"generated_text = text_generator(\"Today is a rainy day in London\",\n                                truncation=True,\n                                num_return_sequences = 3)\nprint(\"Generated_text:\\n \", generated_text[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-27T13:58:19.196934Z","iopub.execute_input":"2024-10-27T13:58:19.197669Z","iopub.status.idle":"2024-10-27T13:58:20.795751Z","shell.execute_reply.started":"2024-10-27T13:58:19.197628Z","shell.execute_reply":"2024-10-27T13:58:20.794696Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"Generated_text:\n  Today is a rainy day in London. People take my phones off, sometimes they call me and don't know where to go. I'm not in any danger of anything but a serious car accident as I was walking home when I arrived. I'll\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Question Answering","metadata":{}},{"cell_type":"code","source":"qa_model = pipeline(\"question-answering\")\n\nquestion = \"WHat is my job?\"\ncontext = \"I am developing AI models with python and data science\"","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:02:28.513343Z","iopub.execute_input":"2024-10-27T14:02:28.514109Z","iopub.status.idle":"2024-10-27T14:02:32.893892Z","shell.execute_reply.started":"2024-10-27T14:02:28.514044Z","shell.execute_reply":"2024-10-27T14:02:32.892943Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\nUsing a pipeline without specifying a model name and revision in production is not recommended.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1925fbee258246d6869fc697fd9ea4b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/261M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e466796340ca481da5b3b23f70a3c817"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3cbd9bd3fe624acda1dc3fafd1a07e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0406f6f4641a4b5086eeb0ed160cbc8e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d22de4c2de2e429db5779bdaffe42bd7"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"qa_model(question=question,context=context)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:02:53.619185Z","iopub.execute_input":"2024-10-27T14:02:53.619933Z","iopub.status.idle":"2024-10-27T14:02:53.692516Z","shell.execute_reply.started":"2024-10-27T14:02:53.619889Z","shell.execute_reply":"2024-10-27T14:02:53.691566Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"{'score': 0.4662597179412842,\n 'start': 5,\n 'end': 25,\n 'answer': 'developing AI models'}"},"metadata":{}}]},{"cell_type":"markdown","source":"## Tokenization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:26:49.301513Z","iopub.execute_input":"2024-10-27T14:26:49.301940Z","iopub.status.idle":"2024-10-27T14:26:49.307423Z","shell.execute_reply.started":"2024-10-27T14:26:49.301898Z","shell.execute_reply":"2024-10-27T14:26:49.306355Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"model_name2 = \"nlptown/bert-base-multilingual-uncased-sentiment\"\nmymodel2 = AutoModelForSequenceClassification.from_pretrained(model_name2)\nmytokenizer2 = AutoTokenizer.from_pretrained(model_name2)\n\nclassifier = pipeline(task=\"sentiment-analysis\",model=mymodel2, tokenizer= mytokenizer2)\nres = classifier(\"I was so not happy with the Barbie Movie\")\nprint(res)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:29:28.785788Z","iopub.execute_input":"2024-10-27T14:29:28.786212Z","iopub.status.idle":"2024-10-27T14:29:36.937396Z","shell.execute_reply.started":"2024-10-27T14:29:28.786171Z","shell.execute_reply":"2024-10-27T14:29:36.936307Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bfe23467f342bcaf553f88e5ec8f67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/669M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"413218ef234b4de4a41e8f8f7f49b2cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/39.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634e6267df7544fdb93458b54f9bc962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28702c524eae429db87bf44261464eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9094bedd60364a4d88d602fb5062c852"}},"metadata":{}},{"name":"stderr","text":"Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"},{"name":"stdout","text":"[{'label': '2 stars', 'score': 0.5099301934242249}]\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Load a pre-trained tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\n# Example text\ntext = \"I was so not happy with the Barbie Movie\"\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:31:21.231622Z","iopub.execute_input":"2024-10-27T14:31:21.232074Z","iopub.status.idle":"2024-10-27T14:31:24.124697Z","shell.execute_reply.started":"2024-10-27T14:31:21.232019Z","shell.execute_reply":"2024-10-27T14:31:24.123598Z"},"trusted":true},"execution_count":30,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856df6a2ff3e41178493488af2ac2284"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7014c609063347e3904b679186ec84a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"895d7af0440b44aa9087ec2e8add4319"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e5af93b06fc4938acea3bd29ef37d8c"}},"metadata":{}},{"name":"stdout","text":"Tokens: ['i', 'was', 'so', 'not', 'happy', 'with', 'the', 'barbie', 'movie']\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(\"Input IDs:\", input_ids)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:32:27.783824Z","iopub.execute_input":"2024-10-27T14:32:27.784276Z","iopub.status.idle":"2024-10-27T14:32:27.790115Z","shell.execute_reply.started":"2024-10-27T14:32:27.784233Z","shell.execute_reply":"2024-10-27T14:32:27.789106Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Input IDs: [1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185]\n","output_type":"stream"}]},{"cell_type":"code","source":"# Encode the text (tokenization + converting to input IDs)\nencoded_input = tokenizer(text)\nprint(\"Encoded Input:\", encoded_input)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:33:29.230158Z","iopub.execute_input":"2024-10-27T14:33:29.231148Z","iopub.status.idle":"2024-10-27T14:33:29.236970Z","shell.execute_reply.started":"2024-10-27T14:33:29.231106Z","shell.execute_reply":"2024-10-27T14:33:29.235948Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Encoded Input: {'input_ids': [101, 1045, 2001, 2061, 2025, 3407, 2007, 1996, 22635, 3185, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Decode the text\ndecoded_output = tokenizer.decode(input_ids)\nprint(\"Decode Output: \", decoded_output)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:33:43.077373Z","iopub.execute_input":"2024-10-27T14:33:43.077770Z","iopub.status.idle":"2024-10-27T14:33:43.083557Z","shell.execute_reply.started":"2024-10-27T14:33:43.077732Z","shell.execute_reply":"2024-10-27T14:33:43.082421Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Decode Output:  i was so not happy with the barbie movie\n","output_type":"stream"}]},{"cell_type":"code","source":"\nfrom transformers import AutoTokenizer\n\n# Load a pre-trained tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n\n# Example text\ntext = \"I was so not happy with the Barbie Movie\"\n\n# Tokenize the text\ntokens = tokenizer.tokenize(text)\nprint(\"Tokens:\", tokens)\n\n# Convert tokens to input IDs\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\nprint(\"Input IDs:\", input_ids)\n\n# Encode the text (tokenization + converting to input IDs)\nencoded_input = tokenizer(text)\nprint(\"Encoded Input:\", encoded_input)\n\n# Decode the text\ndecoded_output = tokenizer.decode(input_ids)\nprint(\"Decode Output: \", decoded_output)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T14:33:54.873007Z","iopub.execute_input":"2024-10-27T14:33:54.873949Z","iopub.status.idle":"2024-10-27T14:33:58.262929Z","shell.execute_reply.started":"2024-10-27T14:33:54.873901Z","shell.execute_reply":"2024-10-27T14:33:58.261940Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4758d65e0b1042e1bb61a3d694f530db"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e163707e8e0741f8a854a992a0468135"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632f068896574b6fba0ee1eb5df4ddf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/436k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45f1737f11a84cfebddf5aaa849b0f22"}},"metadata":{}},{"name":"stdout","text":"Tokens: ['I', 'was', 'so', 'not', 'happy', 'with', 'the', 'Barbie', 'Movie']\nInput IDs: [146, 1108, 1177, 1136, 2816, 1114, 1103, 25374, 8275]\nEncoded Input: {'input_ids': [101, 146, 1108, 1177, 1136, 2816, 1114, 1103, 25374, 8275, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\nDecode Output:  I was so not happy with the Barbie Movie\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine Tuning IMDB","metadata":{}},{"cell_type":"markdown","source":"### Step 1: Install Necessary Libraries","metadata":{}},{"cell_type":"code","source":"!pip install datasets","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:23:27.084128Z","iopub.execute_input":"2024-10-27T16:23:27.084872Z","iopub.status.idle":"2024-10-27T16:23:40.660612Z","shell.execute_reply.started":"2024-10-27T16:23:27.084830Z","shell.execute_reply":"2024-10-27T16:23:40.659577Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.15.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: requests>=2.32.2 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: huggingface-hub>=0.22.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.25.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 2: Load and Prepare the Dataset","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('imdb')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:24:25.913932Z","iopub.execute_input":"2024-10-27T16:24:25.914734Z","iopub.status.idle":"2024-10-27T16:24:33.283383Z","shell.execute_reply.started":"2024-10-27T16:24:25.914690Z","shell.execute_reply":"2024-10-27T16:24:33.282610Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/7.81k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ec2f04209e547d0b1c92861b12a2b68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4af9a087be9842b18e4927420c24845d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d77c7e95b73646aabc329fdbf62165d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d6b15d9ea9c4642a359ef6061c2925e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c9d658ca7574597b4f29b209ba26256"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"614cd05b410d4fe18c945577ce739731"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82589e6f32b49d2a5138bf8221f84d9"}},"metadata":{}}]},{"cell_type":"code","source":"dataset","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:24:45.987494Z","iopub.execute_input":"2024-10-27T16:24:45.988533Z","iopub.status.idle":"2024-10-27T16:24:45.995076Z","shell.execute_reply.started":"2024-10-27T16:24:45.988492Z","shell.execute_reply":"2024-10-27T16:24:45.994179Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:25:12.600520Z","iopub.execute_input":"2024-10-27T16:25:12.600929Z","iopub.status.idle":"2024-10-27T16:25:12.610640Z","shell.execute_reply.started":"2024-10-27T16:25:12.600892Z","shell.execute_reply":"2024-10-27T16:25:12.609712Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 3: Preprocess the Data","metadata":{}},{"cell_type":"code","source":"# Tokenize the dataset using the tokenizer associated with the pretrained model\n\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'],padding=\"max_length\",truncation=True)\n\ntokenized_datasets = dataset.map(tokenize_function,batched=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:28:47.395834Z","iopub.execute_input":"2024-10-27T16:28:47.396515Z","iopub.status.idle":"2024-10-27T16:30:04.934585Z","shell.execute_reply.started":"2024-10-27T16:28:47.396477Z","shell.execute_reply":"2024-10-27T16:30:04.933558Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423d639fe6f54a3ba20754b5cb884c82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50fae04c65794ef9a0176470f4c7e864"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9e14a424ea64c5084ff8882a77af80a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df9c742ed75f448c9a481ce3659347c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"592f45369b6e478e809ed035c0abd23f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab69a9893dad4ad3ab0e39172525cb25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/25000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3048b0337aec42538ac212d859231c35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e635c692dafc478d836a6374280634ed"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:30:04.936473Z","iopub.execute_input":"2024-10-27T16:30:04.937067Z","iopub.status.idle":"2024-10-27T16:30:04.943310Z","shell.execute_reply.started":"2024-10-27T16:30:04.937020Z","shell.execute_reply":"2024-10-27T16:30:04.942458Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 50000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets[\"train\"][0]","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:30:09.917571Z","iopub.execute_input":"2024-10-27T16:30:09.918445Z","iopub.status.idle":"2024-10-27T16:30:09.953964Z","shell.execute_reply.started":"2024-10-27T16:30:09.918402Z","shell.execute_reply":"2024-10-27T16:30:09.952863Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n 'label': 0,\n 'input_ids': [101,\n  1045,\n  12524,\n  1045,\n  2572,\n  8025,\n  1011,\n  3756,\n  2013,\n  2026,\n  2678,\n  3573,\n  2138,\n  1997,\n  2035,\n  1996,\n  6704,\n  2008,\n  5129,\n  2009,\n  2043,\n  2009,\n  2001,\n  2034,\n  2207,\n  1999,\n  3476,\n  1012,\n  1045,\n  2036,\n  2657,\n  2008,\n  2012,\n  2034,\n  2009,\n  2001,\n  8243,\n  2011,\n  1057,\n  1012,\n  1055,\n  1012,\n  8205,\n  2065,\n  2009,\n  2412,\n  2699,\n  2000,\n  4607,\n  2023,\n  2406,\n  1010,\n  3568,\n  2108,\n  1037,\n  5470,\n  1997,\n  3152,\n  2641,\n  1000,\n  6801,\n  1000,\n  1045,\n  2428,\n  2018,\n  2000,\n  2156,\n  2023,\n  2005,\n  2870,\n  1012,\n  1026,\n  7987,\n  1013,\n  1028,\n  1026,\n  7987,\n  1013,\n  1028,\n  1996,\n  5436,\n  2003,\n  8857,\n  2105,\n  1037,\n  2402,\n  4467,\n  3689,\n  3076,\n  2315,\n  14229,\n  2040,\n  4122,\n  2000,\n  4553,\n  2673,\n  2016,\n  2064,\n  2055,\n  2166,\n  1012,\n  1999,\n  3327,\n  2016,\n  4122,\n  2000,\n  3579,\n  2014,\n  3086,\n  2015,\n  2000,\n  2437,\n  2070,\n  4066,\n  1997,\n  4516,\n  2006,\n  2054,\n  1996,\n  2779,\n  25430,\n  14728,\n  2245,\n  2055,\n  3056,\n  2576,\n  3314,\n  2107,\n  2004,\n  1996,\n  5148,\n  2162,\n  1998,\n  2679,\n  3314,\n  1999,\n  1996,\n  2142,\n  2163,\n  1012,\n  1999,\n  2090,\n  4851,\n  8801,\n  1998,\n  6623,\n  7939,\n  4697,\n  3619,\n  1997,\n  8947,\n  2055,\n  2037,\n  10740,\n  2006,\n  4331,\n  1010,\n  2016,\n  2038,\n  3348,\n  2007,\n  2014,\n  3689,\n  3836,\n  1010,\n  19846,\n  1010,\n  1998,\n  2496,\n  2273,\n  1012,\n  1026,\n  7987,\n  1013,\n  1028,\n  1026,\n  7987,\n  1013,\n  1028,\n  2054,\n  8563,\n  2033,\n  2055,\n  1045,\n  2572,\n  8025,\n  1011,\n  3756,\n  2003,\n  2008,\n  2871,\n  2086,\n  3283,\n  1010,\n  2023,\n  2001,\n  2641,\n  26932,\n  1012,\n  2428,\n  1010,\n  1996,\n  3348,\n  1998,\n  16371,\n  25469,\n  5019,\n  2024,\n  2261,\n  1998,\n  2521,\n  2090,\n  1010,\n  2130,\n  2059,\n  2009,\n  1005,\n  1055,\n  2025,\n  2915,\n  2066,\n  2070,\n  10036,\n  2135,\n  2081,\n  22555,\n  2080,\n  1012,\n  2096,\n  2026,\n  2406,\n  3549,\n  2568,\n  2424,\n  2009,\n  16880,\n  1010,\n  1999,\n  4507,\n  3348,\n  1998,\n  16371,\n  25469,\n  2024,\n  1037,\n  2350,\n  18785,\n  1999,\n  4467,\n  5988,\n  1012,\n  2130,\n  13749,\n  7849,\n  24544,\n  1010,\n  15835,\n  2037,\n  3437,\n  2000,\n  2204,\n  2214,\n  2879,\n  2198,\n  4811,\n  1010,\n  2018,\n  3348,\n  5019,\n  1999,\n  2010,\n  3152,\n  1012,\n  1026,\n  7987,\n  1013,\n  1028,\n  1026,\n  7987,\n  1013,\n  1028,\n  1045,\n  2079,\n  4012,\n  3549,\n  2094,\n  1996,\n  16587,\n  2005,\n  1996,\n  2755,\n  2008,\n  2151,\n  3348,\n  3491,\n  1999,\n  1996,\n  2143,\n  2003,\n  3491,\n  2005,\n  6018,\n  5682,\n  2738,\n  2084,\n  2074,\n  2000,\n  5213,\n  2111,\n  1998,\n  2191,\n  2769,\n  2000,\n  2022,\n  3491,\n  1999,\n  26932,\n  12370,\n  1999,\n  2637,\n  1012,\n  1045,\n  2572,\n  8025,\n  1011,\n  3756,\n  2003,\n  1037,\n  2204,\n  2143,\n  2005,\n  3087,\n  5782,\n  2000,\n  2817,\n  1996,\n  6240,\n  1998,\n  14629,\n  1006,\n  2053,\n  26136,\n  3832,\n  1007,\n  1997,\n  4467,\n  5988,\n  1012,\n  2021,\n  2428,\n  1010,\n  2023,\n  2143,\n  2987,\n  1005,\n  1056,\n  2031,\n  2172,\n  1997,\n  1037,\n  5436,\n  1012,\n  102,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'token_type_ids': [0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0]}"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 4: Set Up the Training Arguments","metadata":{}},{"cell_type":"code","source":"# Specify the hyperparameters and training settings\n\nfrom transformers import TrainingArguments \n\ntraining_args = TrainingArguments(\n    output_dir='/kaggle/working/results',          # Output directory\n    eval_strategy =\"epoch\",     # Evaluate every epoch\n    learning_rate=2e-5,              # Learning rate\n    per_device_train_batch_size=16,  # Batch size for training\n    per_device_eval_batch_size=16,   # Batch size for evaluation\n    num_train_epochs=1,              # Number of training epochs\n    weight_decay=0.01,               # Strength of weight decay\n)\ntraining_args","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:33:58.037514Z","iopub.execute_input":"2024-10-27T16:33:58.038236Z","iopub.status.idle":"2024-10-27T16:34:13.705076Z","shell.execute_reply.started":"2024-10-27T16:33:58.038195Z","shell.execute_reply":"2024-10-27T16:34:13.704113Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=2,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=None,\neval_strategy=epoch,\neval_use_gather_object=False,\nevaluation_strategy=None,\nfp16=False,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=1,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=passive,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=/kaggle/working/results/runs/Oct27_16-33-58_6f706c0d034e,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=500,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=linear,\nmax_grad_norm=1.0,\nmax_steps=-1,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=1,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=/kaggle/working/results,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=16,\nper_device_train_batch_size=16,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=/kaggle/working/results,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=True,\nsave_steps=500,\nsave_strategy=steps,\nsave_total_limit=None,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=0,\nweight_decay=0.01,\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 5: Initialize the Model","metadata":{}},{"cell_type":"code","source":"# Load the pre-trained model and define the training procedure\n\nfrom transformers import AutoModelForSequenceClassification, Trainer\n\n# Load the pretrained model \nmodel = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n\n# Initialize the Trainer\ntrainer = Trainer(\n     model = model,\n     args = training_args,\n     train_dataset = tokenized_datasets['train'],\n    eval_dataset = tokenized_datasets['test']\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:38:05.670267Z","iopub.execute_input":"2024-10-27T16:38:05.671198Z","iopub.status.idle":"2024-10-27T16:38:09.582826Z","shell.execute_reply.started":"2024-10-27T16:38:05.671154Z","shell.execute_reply":"2024-10-27T16:38:09.582041Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4306476c80534dba88c277b8dbefbddf"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 6: Train the Model ","metadata":{}},{"cell_type":"code","source":"# Fine tune the pre trained model on our specific dataset. \n\n# Train the model \ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-27T16:39:08.530230Z","iopub.execute_input":"2024-10-27T16:39:08.530639Z","iopub.status.idle":"2024-10-27T17:09:09.871653Z","shell.execute_reply.started":"2024-10-27T16:39:08.530595Z","shell.execute_reply":"2024-10-27T17:09:09.870835Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113940877776764, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4f8e0652d844c298570c8a9fd35099f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241027_164025-d92k6ycq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/prajapatramdhan2001-rp/huggingface/runs/d92k6ycq' target=\"_blank\">/kaggle/working/results</a></strong> to <a href='https://wandb.ai/prajapatramdhan2001-rp/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/prajapatramdhan2001-rp/huggingface' target=\"_blank\">https://wandb.ai/prajapatramdhan2001-rp/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/prajapatramdhan2001-rp/huggingface/runs/d92k6ycq' target=\"_blank\">https://wandb.ai/prajapatramdhan2001-rp/huggingface/runs/d92k6ycq</a>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [782/782 28:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.265200</td>\n      <td>0.171751</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=782, training_loss=0.2414768004356443, metrics={'train_runtime': 1799.5584, 'train_samples_per_second': 13.892, 'train_steps_per_second': 0.435, 'total_flos': 6577776384000000.0, 'train_loss': 0.2414768004356443, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Step 7: Evaluate the Model ","metadata":{}},{"cell_type":"code","source":"# Assess the model's performance on a validation set\n\n# Evaluate the model\nresults = trainer.evaluate()\nprint(results)\n     ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:14:45.421706Z","iopub.execute_input":"2024-10-27T17:14:45.422661Z","iopub.status.idle":"2024-10-27T17:22:03.236660Z","shell.execute_reply.started":"2024-10-27T17:14:45.422607Z","shell.execute_reply":"2024-10-27T17:22:03.235693Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='782' max='782' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [782/782 07:17]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.1717512309551239, 'eval_runtime': 437.8012, 'eval_samples_per_second': 57.104, 'eval_steps_per_second': 1.786, 'epoch': 1.0}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Step 8: Save the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"# save the model \nmodel.save_pretrained('/kaggle/working/fine-tuned-model')\ntokenizer.save_pretrained('/kaggle/working/fin-tuned-tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:24:37.953507Z","iopub.execute_input":"2024-10-27T17:24:37.954191Z","iopub.status.idle":"2024-10-27T17:24:38.873395Z","shell.execute_reply.started":"2024-10-27T17:24:37.954151Z","shell.execute_reply":"2024-10-27T17:24:38.872465Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/fin-tuned-tokenizer/tokenizer_config.json',\n '/kaggle/working/fin-tuned-tokenizer/special_tokens_map.json',\n '/kaggle/working/fin-tuned-tokenizer/vocab.txt',\n '/kaggle/working/fin-tuned-tokenizer/added_tokens.json',\n '/kaggle/working/fin-tuned-tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"markdown","source":"# ArXiv Project ","metadata":{}},{"cell_type":"code","source":"!pip install ArXiv","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:25:06.533797Z","iopub.execute_input":"2024-10-27T17:25:06.534180Z","iopub.status.idle":"2024-10-27T17:25:21.114491Z","shell.execute_reply.started":"2024-10-27T17:25:06.534144Z","shell.execute_reply":"2024-10-27T17:25:21.113382Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting ArXiv\n  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\nCollecting feedparser~=6.0.10 (from ArXiv)\n  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: requests~=2.32.0 in /opt/conda/lib/python3.10/site-packages (from ArXiv) (2.32.3)\nCollecting sgmllib3k (from feedparser~=6.0.10->ArXiv)\n  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->ArXiv) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->ArXiv) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->ArXiv) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests~=2.32.0->ArXiv) (2024.8.30)\nDownloading arxiv-2.1.3-py3-none-any.whl (11 kB)\nDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n  Building wheel for sgmllib3k (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6049 sha256=0fec3564cdc6fe14d607bd20e73dac2240f84b780790333a83ad10e87c3ed9b5\n  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\nSuccessfully built sgmllib3k\nInstalling collected packages: sgmllib3k, feedparser, ArXiv\nSuccessfully installed ArXiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import arxiv \nimport pandas as pd ","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:25:21.117045Z","iopub.execute_input":"2024-10-27T17:25:21.117749Z","iopub.status.idle":"2024-10-27T17:25:21.170723Z","shell.execute_reply.started":"2024-10-27T17:25:21.117697Z","shell.execute_reply":"2024-10-27T17:25:21.169714Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Query to fetch AI-related papers\nquery = 'ai OR artificial intelligence OR machine learning'\nsearch = arxiv.Search(query=query, max_results=10, sort_by=arxiv.SortCriterion.SubmittedDate)\n\n# Fetch papers\npapers = []\nfor result in search.results():\n    papers.append({\n      'published': result.published,\n        'title': result.title,\n        'abstract': result.summary,\n        'categories': result.categories\n    })\n\n# Convert to DataFrame\ndf = pd.DataFrame(papers)\n\npd.set_option('display.max_colwidth', None)\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:25:21.172872Z","iopub.execute_input":"2024-10-27T17:25:21.173230Z","iopub.status.idle":"2024-10-27T17:25:23.321776Z","shell.execute_reply.started":"2024-10-27T17:25:21.173197Z","shell.execute_reply":"2024-10-27T17:25:23.320833Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3709015367.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n  for result in search.results():\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"                  published  \\\n0 2024-10-24 17:59:58+00:00   \n1 2024-10-24 17:59:38+00:00   \n2 2024-10-24 17:59:31+00:00   \n3 2024-10-24 17:59:30+00:00   \n4 2024-10-24 17:59:23+00:00   \n5 2024-10-24 17:59:21+00:00   \n6 2024-10-24 17:59:16+00:00   \n7 2024-10-24 17:59:14+00:00   \n8 2024-10-24 17:58:31+00:00   \n9 2024-10-24 17:58:22+00:00   \n\n                                                                                                                 title  \\\n0                                         PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views   \n1                                                                    CAMEL-Bench: A Comprehensive Arabic LMM Benchmark   \n2                                                   Unbounded: A Generative Infinite Game of Character Life Simulation   \n3                                  3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation   \n4                                                                         Tuning-free coreset Markov chain Monte Carlo   \n5  Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques   \n6                                              ConceptDrift: Uncovering Biases through the Lens of Foundational Models   \n7                                                                    Self-Improving Autonomous Underwater Manipulation   \n8                                       Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms   \n9             Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             abstract  \\\n0                                                              We propose PixelGaussian, an efficient feed-forward framework for learning\\ngeneralizable 3D Gaussian reconstruction from arbitrary views. Most existing\\nmethods rely on uniform pixel-wise Gaussian representations, which learn a\\nfixed number of 3D Gaussians for each view and cannot generalize well to more\\ninput views. Differently, our PixelGaussian dynamically adapts both the\\nGaussian distribution and quantity based on geometric complexity, leading to\\nmore efficient representations and significant improvements in reconstruction\\nquality. Specifically, we introduce a Cascade Gaussian Adapter to adjust\\nGaussian distribution according to local geometry complexity identified by a\\nkeypoint scorer. CGA leverages deformable attention in context-aware\\nhypernetworks to guide Gaussian pruning and splitting, ensuring accurate\\nrepresentation in complex regions while reducing redundancy. Furthermore, we\\ndesign a transformer-based Iterative Gaussian Refiner module that refines\\nGaussian representations through direct image-Gaussian interactions. Our\\nPixelGaussian can effectively reduce Gaussian redundancy as input views\\nincrease. We conduct extensive experiments on the large-scale ACID and\\nRealEstate10K datasets, where our method achieves state-of-the-art performance\\nwith good generalization to various numbers of views. Code:\\nhttps://github.com/Barrybarry-Smith/PixelGaussian.   \n1                                                                                     Recent years have witnessed a significant interest in developing large\\nmultimodal models (LMMs) capable of performing various visual reasoning and\\nunderstanding tasks. This has led to the introduction of multiple LMM\\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\\nevaluation benchmarks are predominantly English-centric. In this work, we\\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\\nrepresent a large population of over 400 million speakers. The proposed\\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\\nsub-domains including, multi-image understanding, complex visual perception,\\nhandwritten document understanding, video understanding, medical imaging, plant\\ndiseases, and remote sensing-based land use understanding to evaluate broad\\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\\nthat are filtered from a larger pool of samples, where the quality is manually\\nverified by native speakers to ensure reliable model assessment. We conduct\\nevaluations of both closed-source, including GPT-4 series, and open-source\\nLMMs. Our analysis reveals the need for substantial improvement, especially\\namong the best open-source models, with even the closed-source GPT-4o achieving\\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.   \n2  We introduce the concept of a generative infinite game, a video game that\\ntranscends the traditional boundaries of finite, hard-coded systems by using\\ngenerative models. Inspired by James P. Carse's distinction between finite and\\ninfinite games, we leverage recent advances in generative AI to create\\nUnbounded: a game of character life simulation that is fully encapsulated in\\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\\nsimulations and allows you to interact with your autonomous virtual character\\nin a virtual world by feeding, playing with and guiding it - with open-ended\\nmechanics generated by an LLM, some of which can be emergent. In order to\\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\\ngeneration domains. Specifically, we present: (1) a specialized, distilled\\nlarge language model (LLM) that dynamically generates game mechanics,\\nnarratives, and character interactions in real-time, and (2) a new dynamic\\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\\nconsistent yet flexible visual generation of a character across multiple\\nenvironments. We evaluate our system through both qualitative and quantitative\\nanalysis, showing significant improvements in character life simulation, user\\ninstruction following, narrative coherence, and visual consistency for both\\ncharacters and the environments compared to traditional related approaches.   \n3                                                                                                                                                              Multi-view image diffusion models have significantly advanced open-domain 3D\\nobject generation. However, most existing models rely on 2D network\\narchitectures that lack inherent 3D biases, resulting in compromised geometric\\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\\nmulti-view features into a coherent 3D representation, then re-encodes the\\nrendered RGBD views to augment the pretrained base model through feature\\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\\nbased on Gaussian splatting and a versatile training-free version utilizing\\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\\nnot only greatly enhances the geometry quality of text-to-multi-view models\\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\\nbroad application potential of 3D-Adapter by presenting high quality results in\\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.   \n4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    A Bayesian coreset is a small, weighted subset of a data set that replaces\\nthe full data during inference to reduce computational cost. The\\nstate-of-the-art coreset construction algorithm, Coreset Markov chain Monte\\nCarlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the\\ncoreset posterior to train the coreset weights via stochastic gradient\\noptimization. However, the quality of the constructed coreset, and thus the\\nquality of its posterior approximation, is sensitive to the stochastic\\noptimization learning rate. In this work, we propose a learning-rate-free\\nstochastic gradient optimization procedure, Hot-start Distance over Gradient\\n(Hot DoG), for training coreset weights in Coreset MCMC without user tuning\\neffort. Empirical results demonstrate that Hot DoG provides higher quality\\nposterior approximations than other learning-rate-free stochastic gradient\\nmethods, and performs competitively to optimally-tuned ADAM.   \n5    Cognitive decline is a natural part of aging, often resulting in reduced\\ncognitive abilities. In some cases, however, this decline is more pronounced,\\ntypically due to disorders such as Alzheimer's disease. Early detection of\\nanomalous cognitive decline is crucial, as it can facilitate timely\\nprofessional intervention. While medical data can help in this detection, it\\noften involves invasive procedures. An alternative approach is to employ\\nnon-intrusive techniques such as speech or handwriting analysis, which do not\\nnecessarily affect daily activities. This survey reviews the most relevant\\nmethodologies that use deep learning techniques to automate the cognitive\\ndecline estimation task, including audio, text, and visual processing. We\\ndiscuss the key features and advantages of each modality and methodology,\\nincluding state-of-the-art approaches like Transformer architecture and\\nfoundation models. In addition, we present works that integrate different\\nmodalities to develop multimodal models. We also highlight the most significant\\ndatasets and the quantitative results from studies using these resources. From\\nthis review, several conclusions emerge. In most cases, the textual modality\\nachieves the best results and is the most relevant for detecting cognitive\\ndecline. Moreover, combining various approaches from individual modalities into\\na multimodal model consistently enhances performance across nearly all\\nscenarios.   \n6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Datasets and pre-trained models come with intrinsic biases. Most methods rely\\non spotting them by analysing misclassified samples, in a semi-automated\\nhuman-computer validation. In contrast, we propose ConceptDrift, a method which\\nanalyzes the weights of a linear probe, learned on top a foundational model. We\\ncapitalize on the weight update trajectory, which starts from the embedding of\\nthe textual representation of the class, and proceeds to drift towards\\nembeddings that disclose hidden biases. Different from prior work, with this\\napproach we can pin-point unwanted correlations from a dataset, providing more\\nthan just possible explanations for the wrong predictions. We empirically prove\\nthe efficacy of our method, by significantly improving zero-shot performance\\nwith biased-augmented prompting. Our method is not bounded to a single\\nmodality, and we experiment in this work with both image (Waterbirds, CelebA,\\nNico++) and text datasets (CivilComments).   \n7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Underwater robotic manipulation faces significant challenges due to complex\\nfluid dynamics and unstructured environments, causing most manipulation systems\\nto rely heavily on human teleoperation. In this paper, we introduce AquaBot, a\\nfully autonomous manipulation system that combines behavior cloning from human\\ndemonstrations with self-learning optimization to improve beyond human\\nteleoperation performance. With extensive real-world experiments, we\\ndemonstrate AquaBot's versatility across diverse manipulation tasks, including\\nobject grasping, trash sorting, and rescue retrieval. Our real-world\\nexperiments show that AquaBot's self-optimized policy outperforms a human\\noperator by 41% in speed. AquaBot represents a promising step towards\\nautonomous and self-improving underwater manipulation systems. We open-source\\nboth hardware and software implementation details.   \n8                                                                                                                                                                                                                                                                     Building a generalist model for user interface (UI) understanding is\\nchallenging due to various foundational issues, such as platform diversity,\\nresolution variation, and data limitation. In this paper, we introduce\\nFerret-UI 2, a multimodal large language model (MLLM) designed for universal UI\\nunderstanding across a wide range of platforms, including iPhone, Android,\\niPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI\\n2 introduces three key innovations: support for multiple platform types,\\nhigh-resolution perception through adaptive scaling, and advanced task training\\ndata generation powered by GPT-4o with set-of-mark visual prompting. These\\nadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,\\nmaking it highly versatile and adaptable for the expanding diversity of\\nplatform ecosystems. Extensive empirical experiments on referring, grounding,\\nuser-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE\\nnext-action prediction dataset, and GUI-World multi-platform benchmark\\ndemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also\\nshows strong cross-platform transfer capabilities.   \n9                                                                                                                                                                                                  Large language models (LLMs) have demonstrated great performance across\\nvarious benchmarks, showing potential as general-purpose task solvers. However,\\nas LLMs are typically trained on vast amounts of data, a significant concern in\\ntheir evaluation is data contamination, where overlap between training data and\\nevaluation datasets inflates performance assessments. While multiple approaches\\nhave been developed to identify data contamination, these approaches rely on\\nspecific assumptions that may not hold universally across different settings.\\nTo bridge this gap, we systematically review 47 papers on data contamination\\ndetection, categorize the underlying assumptions, and assess whether they have\\nbeen rigorously validated. We identify and analyze eight categories of\\nassumptions and test three of them as case studies. Our analysis reveals that\\nwhen classifying instances used for pretraining LLMs, detection approaches\\nbased on these three assumptions perform close to random guessing, suggesting\\nthat current LLMs learn data distributions rather than memorizing individual\\ninstances. Overall, this work underscores the importance of approaches clearly\\nstating their underlying assumptions and testing their validity across various\\nscenarios.   \n\n                            categories  \n0                [cs.CV, cs.AI, cs.LG]  \n1  [cs.CV, cs.AI, cs.CL, cs.CY, cs.LG]  \n2  [cs.CV, cs.AI, cs.CL, cs.GR, cs.LG]  \n3                       [cs.CV, cs.AI]  \n4                     [stat.CO, cs.LG]  \n5                       [cs.LG, cs.AI]  \n6                       [cs.AI, cs.LG]  \n7                              [cs.RO]  \n8                [cs.CV, cs.CL, cs.LG]  \n9                              [cs.CL]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>published</th>\n      <th>title</th>\n      <th>abstract</th>\n      <th>categories</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2024-10-24 17:59:58+00:00</td>\n      <td>PixelGaussian: Generalizable 3D Gaussian Reconstruction from Arbitrary Views</td>\n      <td>We propose PixelGaussian, an efficient feed-forward framework for learning\\ngeneralizable 3D Gaussian reconstruction from arbitrary views. Most existing\\nmethods rely on uniform pixel-wise Gaussian representations, which learn a\\nfixed number of 3D Gaussians for each view and cannot generalize well to more\\ninput views. Differently, our PixelGaussian dynamically adapts both the\\nGaussian distribution and quantity based on geometric complexity, leading to\\nmore efficient representations and significant improvements in reconstruction\\nquality. Specifically, we introduce a Cascade Gaussian Adapter to adjust\\nGaussian distribution according to local geometry complexity identified by a\\nkeypoint scorer. CGA leverages deformable attention in context-aware\\nhypernetworks to guide Gaussian pruning and splitting, ensuring accurate\\nrepresentation in complex regions while reducing redundancy. Furthermore, we\\ndesign a transformer-based Iterative Gaussian Refiner module that refines\\nGaussian representations through direct image-Gaussian interactions. Our\\nPixelGaussian can effectively reduce Gaussian redundancy as input views\\nincrease. We conduct extensive experiments on the large-scale ACID and\\nRealEstate10K datasets, where our method achieves state-of-the-art performance\\nwith good generalization to various numbers of views. Code:\\nhttps://github.com/Barrybarry-Smith/PixelGaussian.</td>\n      <td>[cs.CV, cs.AI, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2024-10-24 17:59:38+00:00</td>\n      <td>CAMEL-Bench: A Comprehensive Arabic LMM Benchmark</td>\n      <td>Recent years have witnessed a significant interest in developing large\\nmultimodal models (LMMs) capable of performing various visual reasoning and\\nunderstanding tasks. This has led to the introduction of multiple LMM\\nbenchmarks to evaluate LMMs on different tasks. However, most existing LMM\\nevaluation benchmarks are predominantly English-centric. In this work, we\\ndevelop a comprehensive LMM evaluation benchmark for the Arabic language to\\nrepresent a large population of over 400 million speakers. The proposed\\nbenchmark, named CAMEL-Bench, comprises eight diverse domains and 38\\nsub-domains including, multi-image understanding, complex visual perception,\\nhandwritten document understanding, video understanding, medical imaging, plant\\ndiseases, and remote sensing-based land use understanding to evaluate broad\\nscenario generalizability. Our CAMEL-Bench comprises around 29,036 questions\\nthat are filtered from a larger pool of samples, where the quality is manually\\nverified by native speakers to ensure reliable model assessment. We conduct\\nevaluations of both closed-source, including GPT-4 series, and open-source\\nLMMs. Our analysis reveals the need for substantial improvement, especially\\namong the best open-source models, with even the closed-source GPT-4o achieving\\nan overall score of 62%. Our benchmark and evaluation scripts are open-sourced.</td>\n      <td>[cs.CV, cs.AI, cs.CL, cs.CY, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2024-10-24 17:59:31+00:00</td>\n      <td>Unbounded: A Generative Infinite Game of Character Life Simulation</td>\n      <td>We introduce the concept of a generative infinite game, a video game that\\ntranscends the traditional boundaries of finite, hard-coded systems by using\\ngenerative models. Inspired by James P. Carse's distinction between finite and\\ninfinite games, we leverage recent advances in generative AI to create\\nUnbounded: a game of character life simulation that is fully encapsulated in\\ngenerative models. Specifically, Unbounded draws inspiration from sandbox life\\nsimulations and allows you to interact with your autonomous virtual character\\nin a virtual world by feeding, playing with and guiding it - with open-ended\\nmechanics generated by an LLM, some of which can be emergent. In order to\\ndevelop Unbounded, we propose technical innovations in both the LLM and visual\\ngeneration domains. Specifically, we present: (1) a specialized, distilled\\nlarge language model (LLM) that dynamically generates game mechanics,\\nnarratives, and character interactions in real-time, and (2) a new dynamic\\nregional image prompt Adapter (IP-Adapter) for vision models that ensures\\nconsistent yet flexible visual generation of a character across multiple\\nenvironments. We evaluate our system through both qualitative and quantitative\\nanalysis, showing significant improvements in character life simulation, user\\ninstruction following, narrative coherence, and visual consistency for both\\ncharacters and the environments compared to traditional related approaches.</td>\n      <td>[cs.CV, cs.AI, cs.CL, cs.GR, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2024-10-24 17:59:30+00:00</td>\n      <td>3D-Adapter: Geometry-Consistent Multi-View Diffusion for High-Quality 3D Generation</td>\n      <td>Multi-view image diffusion models have significantly advanced open-domain 3D\\nobject generation. However, most existing models rely on 2D network\\narchitectures that lack inherent 3D biases, resulting in compromised geometric\\nconsistency. To address this challenge, we introduce 3D-Adapter, a plug-in\\nmodule designed to infuse 3D geometry awareness into pretrained image diffusion\\nmodels. Central to our approach is the idea of 3D feedback augmentation: for\\neach denoising step in the sampling loop, 3D-Adapter decodes intermediate\\nmulti-view features into a coherent 3D representation, then re-encodes the\\nrendered RGBD views to augment the pretrained base model through feature\\naddition. We study two variants of 3D-Adapter: a fast feed-forward version\\nbased on Gaussian splatting and a versatile training-free version utilizing\\nneural fields and meshes. Our extensive experiments demonstrate that 3D-Adapter\\nnot only greatly enhances the geometry quality of text-to-multi-view models\\nsuch as Instant3D and Zero123++, but also enables high-quality 3D generation\\nusing the plain text-to-image Stable Diffusion. Furthermore, we showcase the\\nbroad application potential of 3D-Adapter by presenting high quality results in\\ntext-to-3D, image-to-3D, text-to-texture, and text-to-avatar tasks.</td>\n      <td>[cs.CV, cs.AI]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2024-10-24 17:59:23+00:00</td>\n      <td>Tuning-free coreset Markov chain Monte Carlo</td>\n      <td>A Bayesian coreset is a small, weighted subset of a data set that replaces\\nthe full data during inference to reduce computational cost. The\\nstate-of-the-art coreset construction algorithm, Coreset Markov chain Monte\\nCarlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the\\ncoreset posterior to train the coreset weights via stochastic gradient\\noptimization. However, the quality of the constructed coreset, and thus the\\nquality of its posterior approximation, is sensitive to the stochastic\\noptimization learning rate. In this work, we propose a learning-rate-free\\nstochastic gradient optimization procedure, Hot-start Distance over Gradient\\n(Hot DoG), for training coreset weights in Coreset MCMC without user tuning\\neffort. Empirical results demonstrate that Hot DoG provides higher quality\\nposterior approximations than other learning-rate-free stochastic gradient\\nmethods, and performs competitively to optimally-tuned ADAM.</td>\n      <td>[stat.CO, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2024-10-24 17:59:21+00:00</td>\n      <td>Deep Insights into Cognitive Decline: A Survey of Leveraging Non-Intrusive Modalities with Deep Learning Techniques</td>\n      <td>Cognitive decline is a natural part of aging, often resulting in reduced\\ncognitive abilities. In some cases, however, this decline is more pronounced,\\ntypically due to disorders such as Alzheimer's disease. Early detection of\\nanomalous cognitive decline is crucial, as it can facilitate timely\\nprofessional intervention. While medical data can help in this detection, it\\noften involves invasive procedures. An alternative approach is to employ\\nnon-intrusive techniques such as speech or handwriting analysis, which do not\\nnecessarily affect daily activities. This survey reviews the most relevant\\nmethodologies that use deep learning techniques to automate the cognitive\\ndecline estimation task, including audio, text, and visual processing. We\\ndiscuss the key features and advantages of each modality and methodology,\\nincluding state-of-the-art approaches like Transformer architecture and\\nfoundation models. In addition, we present works that integrate different\\nmodalities to develop multimodal models. We also highlight the most significant\\ndatasets and the quantitative results from studies using these resources. From\\nthis review, several conclusions emerge. In most cases, the textual modality\\nachieves the best results and is the most relevant for detecting cognitive\\ndecline. Moreover, combining various approaches from individual modalities into\\na multimodal model consistently enhances performance across nearly all\\nscenarios.</td>\n      <td>[cs.LG, cs.AI]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2024-10-24 17:59:16+00:00</td>\n      <td>ConceptDrift: Uncovering Biases through the Lens of Foundational Models</td>\n      <td>Datasets and pre-trained models come with intrinsic biases. Most methods rely\\non spotting them by analysing misclassified samples, in a semi-automated\\nhuman-computer validation. In contrast, we propose ConceptDrift, a method which\\nanalyzes the weights of a linear probe, learned on top a foundational model. We\\ncapitalize on the weight update trajectory, which starts from the embedding of\\nthe textual representation of the class, and proceeds to drift towards\\nembeddings that disclose hidden biases. Different from prior work, with this\\napproach we can pin-point unwanted correlations from a dataset, providing more\\nthan just possible explanations for the wrong predictions. We empirically prove\\nthe efficacy of our method, by significantly improving zero-shot performance\\nwith biased-augmented prompting. Our method is not bounded to a single\\nmodality, and we experiment in this work with both image (Waterbirds, CelebA,\\nNico++) and text datasets (CivilComments).</td>\n      <td>[cs.AI, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2024-10-24 17:59:14+00:00</td>\n      <td>Self-Improving Autonomous Underwater Manipulation</td>\n      <td>Underwater robotic manipulation faces significant challenges due to complex\\nfluid dynamics and unstructured environments, causing most manipulation systems\\nto rely heavily on human teleoperation. In this paper, we introduce AquaBot, a\\nfully autonomous manipulation system that combines behavior cloning from human\\ndemonstrations with self-learning optimization to improve beyond human\\nteleoperation performance. With extensive real-world experiments, we\\ndemonstrate AquaBot's versatility across diverse manipulation tasks, including\\nobject grasping, trash sorting, and rescue retrieval. Our real-world\\nexperiments show that AquaBot's self-optimized policy outperforms a human\\noperator by 41% in speed. AquaBot represents a promising step towards\\nautonomous and self-improving underwater manipulation systems. We open-source\\nboth hardware and software implementation details.</td>\n      <td>[cs.RO]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2024-10-24 17:58:31+00:00</td>\n      <td>Ferret-UI 2: Mastering Universal User Interface Understanding Across Platforms</td>\n      <td>Building a generalist model for user interface (UI) understanding is\\nchallenging due to various foundational issues, such as platform diversity,\\nresolution variation, and data limitation. In this paper, we introduce\\nFerret-UI 2, a multimodal large language model (MLLM) designed for universal UI\\nunderstanding across a wide range of platforms, including iPhone, Android,\\niPad, Webpage, and AppleTV. Building on the foundation of Ferret-UI, Ferret-UI\\n2 introduces three key innovations: support for multiple platform types,\\nhigh-resolution perception through adaptive scaling, and advanced task training\\ndata generation powered by GPT-4o with set-of-mark visual prompting. These\\nadvancements enable Ferret-UI 2 to perform complex, user-centered interactions,\\nmaking it highly versatile and adaptable for the expanding diversity of\\nplatform ecosystems. Extensive empirical experiments on referring, grounding,\\nuser-centric advanced tasks (comprising 9 subtasks $\\times$ 5 platforms), GUIDE\\nnext-action prediction dataset, and GUI-World multi-platform benchmark\\ndemonstrate that Ferret-UI 2 significantly outperforms Ferret-UI, and also\\nshows strong cross-platform transfer capabilities.</td>\n      <td>[cs.CV, cs.CL, cs.LG]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2024-10-24 17:58:22+00:00</td>\n      <td>Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions</td>\n      <td>Large language models (LLMs) have demonstrated great performance across\\nvarious benchmarks, showing potential as general-purpose task solvers. However,\\nas LLMs are typically trained on vast amounts of data, a significant concern in\\ntheir evaluation is data contamination, where overlap between training data and\\nevaluation datasets inflates performance assessments. While multiple approaches\\nhave been developed to identify data contamination, these approaches rely on\\nspecific assumptions that may not hold universally across different settings.\\nTo bridge this gap, we systematically review 47 papers on data contamination\\ndetection, categorize the underlying assumptions, and assess whether they have\\nbeen rigorously validated. We identify and analyze eight categories of\\nassumptions and test three of them as case studies. Our analysis reveals that\\nwhen classifying instances used for pretraining LLMs, detection approaches\\nbased on these three assumptions perform close to random guessing, suggesting\\nthat current LLMs learn data distributions rather than memorizing individual\\ninstances. Overall, this work underscores the importance of approaches clearly\\nstating their underlying assumptions and testing their validity across various\\nscenarios.</td>\n      <td>[cs.CL]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\n# Example abstract from API\nfrom transformers import pipeline\n\nabstract = df['abstract'][0]\n\nsummarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n\n# Summarization\nsummarization_result = summarizer(abstract)","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:25:49.611533Z","iopub.execute_input":"2024-10-27T17:25:49.612497Z","iopub.status.idle":"2024-10-27T17:26:11.743495Z","shell.execute_reply.started":"2024-10-27T17:25:49.612455Z","shell.execute_reply":"2024-10-27T17:26:11.742479Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bd22cffce25496b91e1ec13082a8384"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26e737ae633c4baf973f1c49b9d5f189"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2522fceb11c74b769855c7eb5de37cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c9182d9ca864a0cae59c2495546ecdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96856b2e5674dfcb777dc5b8666ed84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32f2b9af51f64a029bc2fac762e20396"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nHardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n","output_type":"stream"}]},{"cell_type":"code","source":"\nsummarization_result[0]['summary_text']","metadata":{"execution":{"iopub.status.busy":"2024-10-27T17:26:11.745341Z","iopub.execute_input":"2024-10-27T17:26:11.745665Z","iopub.status.idle":"2024-10-27T17:26:11.752664Z","shell.execute_reply.started":"2024-10-27T17:26:11.745631Z","shell.execute_reply":"2024-10-27T17:26:11.751646Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'Most existing methods rely on uniform pixel-wise Gaussian representations. We introduce a Cascade Gaussian Adapter to adjustGaussian distribution according to local geometry complexity. CGA leverages deformable attention in context-awarehypernetworks to guide Gaussian pruning and splitting. We conduct extensive experiments on the large-scale ACID and RealEstate10K datasets.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}